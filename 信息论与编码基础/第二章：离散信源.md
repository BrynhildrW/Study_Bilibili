# 第二章：离散信源
## 2.1 离散信源的信息熵
### 2.1.1 信源模型
输出消息（符号）个数有限的信源，如二进制码、数字码、摩尔斯码等，称为**离散信源**，可通过离散型随机变量描述，其数学模型是离散型的概率空间（假设信源可输出符号共计 $q$ 个），包括随机变量 $X$ 的可能取值 $a_i$ 以及相应的概率 $P(a_i)$：
$$
    \begin{bmatrix}
        X\\ \ \\ P(x)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        a_1 & a_2 & \cdots & a_q\\
        \ \\
        P(a_1) & P(a_2) & \cdots & P(a_q)
    \end{bmatrix}, \ \ \sum_{i=1}^{q}P(a_i) = 1
    \tag{2-1}
$$
类似模拟语音、模拟视频信号等具有无限多输出状态的信源，称为**连续信源**，可通过连续性随机变量描述。若输出范围定义为 $(a,b)$，其数学模型由随机变量 $X$ 与相应的概率密度函数 $P(x)$ 组成：
$$
    \begin{bmatrix}
        X\\ \ \\ P(x)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        (a,b)\\ \ \\ P(x)\\
    \end{bmatrix}, \ \ \int_{a}^{b}P(x)dx = 1
    \tag{2-2}
$$
在现实世界中，大多数信源每次输出的消息都由一系列符号组成（设共计 $q$ 种符号，每次输出 $N$ 个符号构成的消息），此时信源不再是一维随机变量 $X$，而应当用离散随机向量 $\pmb{X} \in \mathbb{R}^{N}$ 描述：
$$
    \begin{bmatrix}
        \pmb{X}\\ \ \\ P(\pmb{x})\\
    \end{bmatrix} = 
    \begin{bmatrix}
        (a_1,a_1,\cdots,a_1) \triangleq \pmb{a}_1 & (a_1,a_1,\cdots,a_2) \triangleq \pmb{a}_2 & \cdots & (a_N,a_N,\cdots,a_N) \triangleq \pmb{a}_{q^N}\\
        \ \\
        P(\pmb{a}_1) & P(\pmb{a}_2) & \cdots & P(\pmb{a}_{q^N})
    \end{bmatrix}, \ \ \sum_{i=1}^{q^N} P(\pmb{a}_i) = 1
    \tag{2-3}
$$
当信源发出的信号中，各符号（$a_1$、$a_2$、$\cdots$）彼此统计独立时，该信源称为**离散无记忆信源**，其 $N$ 维度随机向量的联合概率分布满足：
$$
    P(\pmb{x}) = \prod_{i=1}^{N} P(x_i=a_{k_i}), \ \ k_i \in \left\{ 1,2,\cdots,q \right\}
    \tag{2-4}
$$
同样地，在实际应用中，信源先后发出的符号之间往往存在一定的相关性。比如输入法中常见的联想词，就是针对这种相关性设计的辅助输入功能。这种信源称为**有记忆信源**，其联合概率需要引入条件概率 $P(x_i|x_{i-1},x_{i-2},\cdots)$ 进行修正。

有记忆信源的符号相关性一般随符号间隔的增大而减弱，比如“**吃**”后面大概率跟的词是“**饭**”，这个组合哪怕在我们没有获取到“**饭**”字的信息时，也能通过合理推断得到这个结果。但是在“**十二点的时候我们去吃饭吧**”这句话中，“**十二点**”和“**饭**”的相关性其实是很弱的，十二点干啥都有可能啊，也不一定是去吃饭对吧。如果对任意信号都完整地考虑每一个符号的前后相关性，那我们就把简单的事情复杂化了。因此在分析过程中，我们可以限制随机序列的记忆长度。譬如一种信源，其每次发出的符号仅与前 $m$ 个符号有关，则该信源称为 $m$ 阶**马尔可夫信源**。更进一步地，当信源输出符号的多维统计特性与时间起点无关时，该信源称为**平稳信源**，平稳的马尔可夫信源又称为**时齐马尔可夫信源**。

### 2.1.2 自信息
对于一个离散信源：
$$
    \begin{bmatrix}
        X\\ \ \\ P(x)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        a_1 & a_2 & \cdots & a_q\\
        \ \\
        P(a_1) & P(a_2) & \cdots & P(a_q)
    \end{bmatrix}, \ \ \sum_{i=1}^{q}P(a_i) = 1
$$
我们通常关注两个主要问题：（1）该信源中各个消息的出现携带了多少信息？（2）整个信源能够输出多少信息？这两个问题对应了信息的两种度量，一是自信息（香农 Shannon 信息），二是信源的信息熵。

记事件 $a_i$ 的自信息为 $I(a_i)$，自信息与 $a_i$ 发生的先验概率 $P(a_i)$ 有关。这里首先需要树立的一个反直觉概念是，$a_i$ 发生的概率越大，其自信息越小：
$$
    I(a_i) = \log_r \dfrac{1}{P(a_i)} = -\log_r P(a_i)
    \tag{2-5}
$$
我们在看到如下定义：“Shannon 信息描述的是信源中各事件出现的先验不确定性”时，总会下意识地将先验概率大小与信息大小划等号。除了严格遵守“不确定性”这样的舶来品描述以外，我们还可以把它理解为“事件 $a_i$ 发生以后提供的信息量”，或者“获取事件 $a_i$ 信息的难度”。当一个信源永远只发送一种符号时，我们不费吹灰之力（难度为 0，自信息为 0）就能知道这个信源的全部情况；当一个卡池只有 0.5% 的概率出稀有角色时，一个不欧也不非的普通玩家想抽干这个卡池的所有角色（获取信源的全部信息）就不得不重氪（难度很高，自信息很大）了。

回到式（2-5），当对数的底数不同时，$I(a_i)$ 的单位不一样：（1）$r=2$ 时，单位为比特（bit）；（2）$r=e$ 时，单位为奈特（nat）；（3）$r=10$ 时，单位为哈特（hart）。根据对数换底关系：
$$
    \log_a x = \dfrac{\log_b x}{\log_b a}
    \tag{2-6}
$$
可知 $1 \ {\rm nat} \approx 1.44 \ {\rm bit}$、$1 \ {\rm hart} \approx 3.32 \ {\rm bit}$。目前较为通用的计量单位是 ${\rm bit}$，各种教材中也不乏将 $\log_2$ 简记为 $\log$ 的情况。在本专栏中，我尽量保证写全每个符号。

对于联合事件集合 $\left\{ XY \right\}$ 中的事件 $x=a_i$、$y=b_j$，其自信息定义为：
$$
    I(a_i b_i) = -\log_2 P(a_i b_j)
    \tag{2-7}
$$
事实上如果把联合事件看成一个单一事件，那么**联合自信息**与自信息的含义则完全一致。

当事件 $x=a_i$、$y=b_j$ 不再组成联合事件、而是构成条件事件时，**条件自信息**定义为：
$$
    I \left( a_i|b_j \right) = -\log_2 P \left( a_i|b_j \right)
    \tag{2-8}
$$
条件自信息的含义与自信息还是比较类似的：在事件 $y=b_j$ 给定的条件下，事件 $x=a_i$ 发生后所得到的信息量。

### 2.1.3 信息熵
$I(a_i)$ 描述了事件 $a_i$ 的信息，而对于一个信源 $X$ 而言，其总共能输出的信息量，或者每个事件出现的平均自信息不再是随机变量，而是一个固定常数，定义为信息熵 $H(X)$，其单位为 ${\rm bit/sig}$：
$$
    E \left[ \log_2 \dfrac{1}{P(a_i)} \right] = \sum_{i=1}^{q} P(a_i) \log_2 \dfrac{1}{P(a_i)} \triangleq H(X), \ \ H(X) = -\sum_{i=1}^{q} P(a_i) \log_2 P(a_i)
    \tag{2-9}
$$
根据式（2-6）可知，当自信息 $I(a_i)$ 的对数底数取其它值 $r$ 时，则信息熵 $H_r(X)$ 与单位 ${\rm bit/sig}$ 均存在对应的转换：
$$
    H_r(X) = \dfrac{H(X)}{\log_2 r}
    \tag{2-10}
$$
以两个信源 $X_1$ 与 $X_2$ 为例：
$$
    \begin{bmatrix}
        X_1\\
        \ \\
        P(X_1)\\
    \end{bmatrix} =
    \begin{bmatrix}
        x_{11} & x_{12}\\
        \ \\
        0.95 & 0.05\\
    \end{bmatrix}, \ \ H(X_1) = -0.95* \log_2 0.95 - 0.05* \log_2 0.05 \approx 0.286 \ {\rm bit/sig}\\
    \ \\
    \begin{bmatrix}
        X_2\\
        \ \\
        P(X_2)\\
    \end{bmatrix} =
    \begin{bmatrix}
        x_{21} & x_{22}\\
        \ \\
        0.47 & 0.53\\
    \end{bmatrix}, \ \ H(X_2) = -0.47* \log_2 0.47 - 0.53* \log_2 0.53 \approx 0.997 \ {\rm bit/sig}
$$
信源中各事件的出现概率越接近，则事先推测某一事件发生的把握越小、不确定性越大、获取事件后得到的信息越多，所以有 $H(X_2) > H(X_1)$。当各事件完全等概率出现时，信源的信息熵达到理论最大值。

### 2.1.4 联合熵与条件熵
联合熵与信息熵的差别在于，把事件 $a_i$ 的概率替换成联合事件 $a_i b_j$ 的概率（$1 \leqslant i \leqslant q$，$1 \leqslant j \leqslant s$），计算联合自信息 $I(a_i b_j)$ 的平均值：
$$
    H(XY) = -\sum_{i=1}^{q} \sum_{j=1}^{s} P(a_i b_j) \log_2 P(a_i b_j)
    \tag{2-11}
$$
类似地，条件熵是条件自信息 $I \left( a_i | b_j \right)$ 的平均值：
$$
    H(X|Y) = -\sum_{j=1}^{r} P \left( b_j \right) \sum_{i=1}^{q} P \left( a_i | b_j \right) \log_2 P \left( a_i | b_j \right) = -\sum_{j=1}^{r} \sum_{i=1}^{q} P \left( a_i b_j \right) \log_2 P \left( a_i | b_j \right)
    \tag{2-12}
$$
例如，在一个有 15 (m) 个黑球，20 (n) 个白球的箱子里无放回抽取两次，第一次颜色为 $X$，第二次颜色为 $Y$，求 $H(Y|X)$：
$$
    \begin{align}
        \notag H(Y|X={\rm black}) &= -\dfrac{m-1}{m+n-1} \log_2 \dfrac{m-1}{m+n-1}- \dfrac{n}{m+n-1} \log_2 \dfrac{n}{m+n-1} \approx 0.977 \ {\rm bit/sig}\\
        \notag \ \\
        \notag H(Y|X={\rm white}) &= -\dfrac{m}{m+n-1} \log_2 \dfrac{m}{m+n-1}- \dfrac{n-1}{m+n-1} \log_2 \dfrac{n-1}{m+n-1} \approx 0.990 \ {\rm bit/sig}\\
        \notag \ \\
        \notag H(Y|X) &= P(X={\rm black}) H(Y|X={\rm black}) + P(X={\rm white}) H(Y|X={\rm white}) \approx 0.985 \ {\rm bit/sig}
    \end{align}
$$
这里额外注意一下 $H(Y) > H(Y|X)$，后边有用：
$$
    \begin{align}
        \notag H(Y) &= - \dfrac{m(m-1)}{(m+n)(m+n-1)} \times \log_2 \dfrac{m(m-1)}{(m+n)(m+n-1)} - \dfrac{mn}{(m+n)(m+n-1)} \times \log_2 \dfrac{mn}{(m+n)(m+n-1)}\\
        \notag \ \\
        \notag &- \dfrac{nm}{(m+n)(m+n-1)} \times \log_2 \dfrac{nm}{(m+n)(m+n-1)} - \dfrac{n(n-1)}{(m+n)(m+n-1)} \times \log_2 \dfrac{n(n-1)}{(m+n)(m+n-1)} \approx 1.970 \ {\rm bit/sig}\\
    \end{align}
$$

## 2.2 熵的基本性质
对于信源 $X$：
$$
    \begin{bmatrix}
        X\\ \ \\ P(x)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_q\\
        \ \\
        P(x_1) & P(x_2) & \cdots & P(x_q)\\
    \end{bmatrix}, \ \ \sum_{i=1}^{q} P(x_i) = 1
$$
其信息熵 $H(X) = -\sum_{i=1}^{q} P(x_i) \log_2 P(x_i)$ 是概率向量 $\pmb{P}(x)= [P(x_1),P(x_2),\cdots,P(x_q)]$ 的函数，且具有以下几条重要性质。下面罗列的内容，参考来源不仅仅是《信息论与编码基础》，还有电子工业出版社的《信息论：基础理论与应用》2.3 节。

1. **非负性**
$$
    H(X) = -\sum_{i=1}^{q} P_i \log_2 P_i \geqslant 0
    \tag{2-13}
$$
由于 $0 \leqslant P_i \leqslant 1$，所以上式很容易证明。需要注意的是，熵的非负性仅对于离散信源成立，对于连续信源是不一定成立的。

2. **确定性**
$$
    H(1,0) = H(1,0,0) = H(1,0,\cdots,0) = 0
    \tag{2-14}
$$
确定性的含义是，当信源 $X$ 存在固定事件时，该信源的熵为 0。这一性质根据自信息的概念也很好理解。

3. **对称性**
$$
    H(P_1,P_2,\cdots,P_q) = H(P_{i_1},P_{i_2},\cdots,P_{i_q})
    \tag{2-15}
$$
其中 $\left\{i_1,i_2,\cdots,i_q \right\}$ 是 $\left\{ 1,2,\cdots,q \right\}$ 的任意一种排列。式（2-15）表示，将相同数量的事件，其概率 $P_i$ 任意互换后，信源的熵不变。信息熵只与随机变量的总体结构有关。

4. **熵的链式法则（Chain Rule）**
对于两个非独立信源 $X$ 与 $Y$：
$$
    \begin{bmatrix}
        X\\ \ \\ P(x)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        x_1 & x_2 & \cdots & x_m\\
        \ \\
        P(x_1) & P(x_2) & \cdots & P(x_m)\\
    \end{bmatrix}, \ \ \sum_{i=1}^{m} P(x_i) = 1\\
    \ \\
    \begin{bmatrix}
        Y\\ \ \\ P(y)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        y_1 & y_2 & \cdots & y_n\\
        \ \\
        P(y_1) & P(y_2) & \cdots & P(y_n)\\
    \end{bmatrix}, \ \ \sum_{i=1}^{n} P(y_i) = 1
$$
其联合信源的概率空间为：
$$
    \begin{bmatrix}
        XY\\ \ \\ P(xy)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        x_1 y_1 & x_1 y_2 & \cdots & x_m y_n\\
        \ \\
        P(x_1 y_1) & P(x_1 y_2) & \cdots & P(x_m y_n)\\
    \end{bmatrix}, \ \ \sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i y_j) = 1
$$
且根据概率论的相关知识，有：
$$
    P(x_i y_j) = P(x_i) P(y_j | x_i)
$$
则联合信源 $XY$ 的信息熵 $H(XY)$ 为：
$$
    \begin{align}
        \notag H(XY) &= -\sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i y_j) \log_2 P(x_i y_j) \\
        \notag &= -\sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i y_j) \log_2 \left[ P(x_i) P(y_j | x_i) \right] \\
        \notag &= -\sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i y_j) \left[ \log_2 P(x_i) + \log_2 P(y_j | x_i) \right] \\
        \notag &= -\sum_{i=1}^{m} P(x_i) \log_2 P(x_i) \underbrace{\sum_{j=1}^{n} P(y_j|x_i)}_{1} - \sum_{i=1}^{m} \sum_{j=1}^{n} P(x_i y_j) \log_2 P(y_j | x_i) \\
        \notag &= H(X) + H(Y|X)
    \end{align}
    \tag{2-16}
$$
熵的**链式法则**又被称为**熵的强可加性**。若两个信源完全独立，即 $H(Y|X)=H(Y)$，则式（2-16）变为 $H(XY)=H(X)+H(Y)$，这是**可加性**，可加性是强可加性的一种特殊情况。

5. **极值性**
当信源中各事件的出现概率均等时，信源具有最大熵。而信源中某一事件的发生占据较大的确定性时，必然引发整个信源的平均不确定性的下降，即信息熵的下降：
$$
    H(P_1,P_2,\cdots,P_q) \leqslant H \left( \dfrac{1}{q},\dfrac{1}{q},\cdots,\dfrac{1}{q} \right) = \log_2 q
    \tag{2-17}
$$

6. **熵的独立界**
$$
    H(X|Y) \leqslant H(X)
    \tag{2-18}
$$
当且仅当 $X$ 与 $Y$ 相互独立时，等号成立。式（2-18）说明了条件作用会使得熵减小，这一结论在 2.1.4 节的例题中得到了证明。进一步拓展可得：条件熵 $H \left(X_N|X_1 X_2 \cdots X_{N-1} \right)$ 随着条件数 $N$ 的增加而下降（或不变），在平均意义上，增加条件会降低一定的不确定性。但是对于联合熵 $H(X_1 X_2 \cdots X_N)$ 而言，增加联合项 $X_N$ 的个数，通常会增加信息熵。对于一个长度为 $N$ 的随机变量序列 $\left\{ X_i \right\}$，该序列的熵随 $N$ 的增长率定义为熵率：
$$
    H_{\infty} = \underset{N \rightarrow \infty} \lim \dfrac{1}{N} H \left(X_1 X_2 \cdots X_N \right)
    \tag{2-19}
$$
以打字机为例。假设每次打字可输出 $M$ 种字符，每种字符输出概率相等，字符序列长度为 $N$，则一共可能输出的字符串种类有 $M^N$ 种。视该打字机为信源，每次输出的字符为随机事件 $X_i$，则有：
$$
    H \left( X_1 X_2 \cdots X_N \right) = \log_2 M^N = N \log_2 M
$$
不难发现 $H \left( X_1 X_2 \cdots X_N \right)$ 是一个关于字符个数 $N$ 单调递增的函数，每新增一个字符，熵增量为 $\log_2 M \ {\rm bit/sig}$，即熵率 $H_{\infty} = \log_2 M \ {\rm bit}$。打字机模型可抽象为**独立同分布**随机变量序列 $\left\{ X_i \right\}$，对这种序列，熵率等于单个字符的熵。更进一步地，我们可以把熵率理解为极限熵。换句话说，增加一个字符，不论编码系统如何精妙，熵增量至少都得有 $H_{\infty}$；对于独立、但不同分布的序列 $\left\{X_i \right\}$，根据可加性可知，其联合熵为 $H \left( X_1 X_2 \cdots X_N \right) = \sum_{i=1}^{N} H(X_i)$。注意此时 $H(X_i)$ 通常不全相等。当式（2-19）所示的极限不存在时，$H_{\infty}$ 无定义；更进一步地，假设随机事件 $X_1$、$X_2$、$\cdots$、$X_N$ 的概率分布服从 $P(X_1)$、$P(X_2)$、$\cdots$、$P(X_N)$，不限定是否独立，是否同分布，则有：
$$
    H(X_1 X_2 \cdots X_N) \leqslant \sum_{i=1}^{N} H(X_i)
    \tag{2-20}
$$
等号仅在 $X_1$、$X_2$、$\cdots$、$X_N$ 相互独立时成立，此为熵的**独立界**。

## 2.3 信源的剩余度
信息熵表示信源每输出一个符号平均所携带的信息量。熵越大，表示信源符号携带信息的效率越高；对于一个具体的信源，其总信息量是固定的。所以信源的熵越大，输出全部信息所需传送的符号就越少，相应地通信效率也就越高，这也是研究信源熵的主要目的之一。上一节讨论的各种信源中，依照复杂性由低到高排序，分别为**离散无记忆信源**、**$m$ 阶马尔可夫信源**、**离散平稳信源**、**一般有记忆信源**等，接下来我们依次分析各类信源。

假设由 $q$ 个符号组成的无记忆信源的熵为 $H_1$，根据上一节信源的极值性可知，当 $q$ 个符号等概率分布时，信源熵取得最大值 $H_0 = \log_2 q$。

对于有记忆信源而言，若输出符号间的相关长度越长，信源熵就越小。例如，以天津大学下辖学院的名称作为信源，则解码出“**医学工程与转化医学研究院**”这一长串名词所需要的符号（信息）事实上远小于任意 12 个中文汉字组合所包含的信息量，因为字符前后的相关性太强了，甚至仅需“**转化**”两字，就能锁定这个名词。假设信源输出序列的字符间相关性（记忆长度）仅限于 2 个符号，则信源的熵为 $H_2$；若记忆长度为 $m+1$，则熵为 $H_{m+1}$，且有：
$$
    \log_2 q = H_0 \geqslant H_1 \geqslant H_2 \geqslant \cdots \geqslant H_{m+1} \geqslant \cdots \geqslant H_{\infty}
    \tag{2-21}
$$
上式说明，等概率分布的、离散无记忆信源的熵 $H_0$ 是所有信源熵中最大的一种，其携带信息的效率最高。因此以 $H_0$ 为参照，提出了信源剩余度的概念。对于极限熵为 $H_{\infty}$ 的 $q$ 元信源而言，其信源剩余度 $r$ 为：
$$
    r = 1 - \dfrac{H_{\infty}}{H_0} = 1 - \dfrac{H_{\infty}}{\log_2 q}
    \tag{2-22}
$$
当信源的最低熵 $H_{\infty}$ 与最大熵 $H_0$ 的差距越大，则信源剩余度 $r$ 越大，换句话说信源编码效率越低（提升空间大），通过合适的压缩方法能够使用更少的符号表达相同含量的信息。