# 第三章：离散信道
信道是通信系统最重要的组成部分，通信的本质就是信息通过信道进行传输，实现异地间的信息交流。研究信道的目的在于研究信道中能传输的最大信息量，即信道容量。在一个典型的通信系统中，信源发出携带一定信息量的信息，转换成适合在信道中传输的信号，之后通过信道传送到接收端。由于干扰和噪声的存在，信道的输入输出往往表现为统计依赖关系。集合**输入信号**、**输出信号**以及两者的**统计依赖关系**，就能确定信道的全部特性。根据以上三种要素，可以对信道做进一步的分类：

根据输入、输出信号的时间特性、取值特性：
（1）**离散信道**：输入、输出均为离散随机变量；
（2）**连续信道**：输入、输出均为连续随机变量；
（3）**半离散信道**：输入、输出中一个为离散型随机变量，另一个为连续型随机变量；
（4）**波形信道**：输入、输出均为时域连续随机信号 $\left\{ x(t) \right\}$ 与 $\left\{ x(t) \right\}$。

根据信道的统计特性：
（1）**恒参信道**：信道的统计特性（噪声干扰、能量功率等）不随时间变化（如卫星信道）；
（2）**随参信道**：信道的统计特性随时间变化。

根据信道用户数量的不同：
（1）**两端（单用户）信道**：只有一个输入端和一个输出端的、单向通信的信道；
（2）**多端（多用户）信道**：在输入或输出端中至少一端有两个以上的用户，同时还能双向通信的信道。进一步可以分为多元接入信道、广播信道等。

根据信道的记忆特性：
（1）**无记忆信道**：信道输出仅与当前输入有关，与过去的输入、输出均无关。类似于信号系统中的因果系统；
（2）**有记忆信道**：信道的输出不仅与当前输入有关，还与过去的输入、输出有关。

在本章中，主要讨论**无记忆**、**恒参**、**单用户**的离散信道，这种理想模型是研究其它信道的基础。

## 3.1 信号疑义度与平均互信息
### 3.1.1 信道模型
离散信道的数学模型可以表示为 $\left\{ \pmb{X}, P(\pmb{y}|\pmb{x}), \pmb{Y} \right\}$：

![离散信道模型](/Study_Bilibili/信息论与编码基础/figures/3-1.png)

输入 $N$ 维随机向量 $\pmb{X}=(X_1,X_2,\cdots,X_N)$ 或输出向量 $\pmb{Y} = (Y_1,Y_2,\cdots,Y_N)$ 由随机变量 $X_i$（或 $Y_i$）组成，其取值 $x_i$ 或 $y_i$ 源自输入（$r$ 维）符号集合 $\pmb{A} = (a_1,a_2,\cdots,a_r)$ 或输出（$s$ 维）符号集合 $\pmb{B} = (b_1,b_2,\cdots,b_s)$；条件概率 $P(\pmb{y}|\pmb{x})$ 表示输入与输出信号之间的统计依赖关系。根据 $P(\pmb{y}|\pmb{x})$ 的不同，离散信道可分为如下三种情况：
（1）无干扰（无噪）信道：$\pmb{Y}$ 与 $\pmb{X}$ 之间有完全不变的对应关系 $\pmb{y} = f(\pmb{x})$ ：
$$
    P(\pmb{y}|\pmb{x}) =
    \begin{cases}
        1, \ \ \pmb{y} = f(\pmb{x})\\
        0, \ \ \pmb{y} \ne f(\pmb{x})\\
    \end{cases}
    \tag{3-1}
$$
（2）有干扰无记忆信道：输出符号与输入符号之间不存在确定对应关系，但具备因果性。其条件概率满足：
$$
    P(\pmb{y}|\pmb{x}) = P(y_1 y_2 \cdots y_N | x_1 x_2 \cdots x_N) = \prod_{i=1}^{N} P(y_i | x_i)
    \tag{3-2}
$$
（3）有干扰有记忆信道：这是更一般的情况，其条件概率更加复杂，需要结合具体情况才能通过公式合理表达。

接下来介绍两种重要的单符号信道，为后续介绍更实用的多符号信道奠定基础：

**二元对称信道**（Binary symmetric channel, BSC）：
BSC 信道的输入、输出符号均取值于 $\left\{ 0,1 \right\}$。记 $a_1=b_1=0$、$a_2=b_2=1$，则各种可能性的转移概率以及信道转移矩阵 $\pmb{P}$ 为：
$$
    P(b_1|a_1) = 1-p, \ \ P(b_2|a_2) = 1-p, \ \ P(b_1|a_2) = p, \ \ P(b_2|a_1) = p \ \ \Longrightarrow \ \ \pmb{P} = \begin{bmatrix}
        1-p & p\\ p & 1-p
    \end{bmatrix}
    \tag{3-3}
$$

![BSC信道](/Study_Bilibili/信息论与编码基础/figures/3-2.png)

BSC 信道的误码率为：
$$
    P_E = P(a_2)P(b_1|a_2) + P(a_1)P(b_2|a_1) = p
    \tag{3-4}
$$
若采用合适的纠错编码技术，可以有效降低信道的误码率，这也是未来设计通信系统的重要研究部分。

**二元删除信道**（Binary erasure channel, BEC）：
BEC 信道的输入 $X$ 取值于 $\left\{ 0,1 \right\}$，输出 $Y$ 取值于 $\left\{ 0,1,2 \right\}$，信道转移矩阵为：
$$
    \pmb{P} = \begin{bmatrix}
        p & 1-p & 0\\
        0 & 1-q & q\\
    \end{bmatrix}
    \tag{3-5}
$$

![BEC信道](/Study_Bilibili/信息论与编码基础/figures/3-3.png)

当信号传输中失真的可能性较大时，接收端不再硬性判别，而是根据失真信息增加了一种模糊中间状态（即删除符号）。通过特定的纠错编码，能够有效恢复出中间状态的正确值。

### 3.1.2 信道疑义度
已知信源 $X$ 的（先验）熵为 $H(X) = -\sum_{x \in X} P(x) \log_2 P(x)$，它代表各符号的平均不确定性。当信源发送了一个符号 $a_i$ 后，收方接收到符号 $b_j$。假如信道没有干扰，则收发过程完成后符号的先验不确定性全部消除；当信道存在干扰时，收到符号 $b_j$ 后，信源发送值是否真的为 $a_i$ 呢？这种统计依赖关系表现为后验概率 $P(a_i|b_j)$。换句话说，此时关于发出 $a_i$ 的不确定性不再是 $\log_2 P(a_i)$，而是 $\log_2 P(a_i|b_j)$。面向发送方所有可能发送的字符（假设有 $s$ 种），则有：
$$
    H(X|b_j) = -\sum_{i=1}^{s} P(a_i|b_j) \log_2 P(a_i|b_j) = -\sum_{x \in X} P(x|b_j) \log_2 P(x|b_j)
$$
进一步考虑接收方可能受到的符号（假设有 $r$ 种），有：
$$
    \begin{align}
        \notag H(X|Y) &= \sum_{j=1}^{r} P(b_j) H(X|b_j)\\
        \notag \ \\
        \notag &=-\sum_{i=1}^{s} \sum_{j=1}^{r} P(b_j) P(a_i|b_j) \log_2 P(a_i|b_j)\\
        \notag \ \\
        \notag &=-\sum_{i=1}^{s} \sum_{j=1}^{r} P(a_ib_j) \log_2 P(a_i|b_j)\\
        \notag \ \\
        \notag &= -\sum_{x \in X} \sum_{y \in Y} P(xy) \log_2 P(x|y)
    \end{align}
    \tag{3-6}
$$
式中的 $H(X|Y)$ 即为信源的信道疑义度，表示接收到输出 $Y$ 的全部符号后，关于发出 $X$ 各符号残留的平均不确定性，这种不确定性是由信道干扰引起的。不难验证，在无干扰情况下信道的输入与输出完全一一对应，残留不确定性完全消除，所以 $P(xy) = 0$、$H(X|Y) = 0$。

当然一般情况下，总会有一些干扰，即 $H(X|Y)>0$。同时根据上一节熵的性质可知，$H(X|Y) \leqslant H(X)$，传输过程结束后总是能获得一些信息、消除一些关于输入的不确定性。此外，当且仅当 $X$ 与 $Y$ 完全独立时，$H(X|Y) = H(X)$。但是假如输入和输出完全独立，那这种信道也没什么实际作用了。

### 3.1.3 平均互信息及其性质
$H(X)$ 表示接收到输出符号之前，关于信源 $X$ 的先验不确定性；而 $H(X|Y)$ 表示接收到符号后残存的不确定性，那么“发送-接受”这个过程获得的信息量应当为两者之差：
$$
    I(X;Y) = H(X) - H(X|Y)
    \tag{3-7}
$$
定义 $I(X;Y)$ 为信道输入 $X$ 与信道输出 $Y$ 之间的平均互信息。$I(X;Y)$ 的具体含义是：接收到每个输出符号 $Y$ 后获得的关于 $X$ 的平均信息量，单位为 $\rm bit/sig$。根据概率论的相关知识，式（3-7）有如下形式的等价表示：
$$
    \begin{align}
        \notag I(X;Y) &= \sum_{xy \in XY} P(xy) \log_2 \dfrac{P(x|y)}{P(x)}\\
        \notag \ \\
        \notag &= \sum_{xy \in XY} P(xy) \log_2 \dfrac{P(xy)}{P(x) P(y)}\\
        \notag \ \\
        \notag &= \sum_{xy \in XY} P(xy) \log_2 \dfrac{P(y|x)}{P(y)}
    \end{align}
    \tag{3-8}
$$
从收端的角度看，收到消息 $y$ 后关于发出事件 $x$ 的信息量为 $x$ 与 $y$ 之间的互信息 $I(x;y)$：
$$
    I(x;y) = \log_2 \dfrac{1}{P(x)} - \log_2 \dfrac{1}{P(x|y)} = \log_2 \dfrac{P(x|y)}{P(x)}
    \tag{3-9}
$$
不难发现，对 $I(x;y)$ 求统计平均后获得的参数正是平均互信息 $I(X;Y)$，在部分文献中亦可记为 $R$。平均互信息存在以下几个重要性质：

（1）**非负性**：
根据具体情况不同，互信息 $I(x;y)$ 的正负性可能会发生变化，但平均互信息 $I(X;Y)$ 一定是非负的：
$$
    I(X;Y) = H(X) - H(X|Y) \geqslant 0
$$
当且仅当 $X$ 与 $Y$ 统计独立时等号成立，其物理意义为：通过一个信道获得的平均信息量不可能为负。

（2）**极值性**：
$$
    H(X|Y) = -\sum_{xy \in XY} P(xy) \log_2 P(x|y) \geqslant 0 \ \ \Longrightarrow \ \ I(X;Y) \leqslant H(X)
$$
该性质的物理意义为：通过信号获得的信息量不可能超过信源本身固有的信息量。当且仅当信道为无损信道（$H(X|Y)=0$）时，才能获得信源的全部信息量。综合非负性可得：
$$
    0 \leqslant I(X;Y) \leqslant H(X)
    \tag{3-10}
$$
等号一般不会同时取得：左等号仅当输入与输出统计独立时取得，右等号仅当信道无损时取得。

（3）**对称性**：
$$
    I(X;Y) = I(Y;X)
    \tag{3-11}
$$
需要注意的是，这种写法并不代表直接互换信源的发送方和接收方：$I(X;Y)$ 表示收到 $Y$ 后获得的关于 $X$ 的信息量，$I(Y;X)$ 表示发出 $X$ 后得到的关于 $Y$ 的信息量。虽然有点绕，但是很明显二者应当是相等的。考虑输入输出独立的情况，则互信息有 $I(X;Y) = I(Y;X) = 0$，即不可能由一个变量获得另一个变量的相关信息，正所谓“独立”嘛；考虑无损信道的情况，则 $I(X;Y) = I(Y;X) = H(X) = H(Y)$，即从一个随机变量能获取到另一个的所有信息。

（4）$I(X;Y)$ **与各类熵的关系**：
根据式（3-8），可以获得对应的三个表达式：
$$
    \begin{align}
        \notag I(X;Y) &= H(X) - H(X|Y)\\
        \notag &= H(Y) - H(Y|X)\\
        \notag &= H(X) + H(Y) - H(XY)\\
    \end{align}
    \tag{3-12}
$$
对式 （3-12）作不同方式的变量转移，能够获得各种各样的计算式，在此不一一列出了，通过 Venn 图能更好地明确平均互信息（$I(X;Y)$）、信源熵（$H(X)$）、信宿熵（$H(Y)$）、联合熵（$H(XY)$）、信道疑义度（$X(Y|X)$）以及信道噪声熵（$H(Y|X)$）的关系：

![互信息与熵的关系图](/Study_Bilibili/信息论与编码基础/figures/3-4.png)

（5）$I(X;Y)$ **的凸函数特性**
$$
    I(X;Y) = \sum_{xy \in XY} P(xy) \log_2 \dfrac{P(y|x)}{P(y)} = \sum_{xy \in XY} P(x) P(y|x) \log_2 \dfrac{P(y|x)}{\sum_{X} P(x)P(y|x)} \triangleq f \left[ P(x),P(y|x) \right]
    \tag{3-13}
$$
换句话说，互信息可表示为关于信源的概率分布 $P(x)$ 与信道转移概率 $P(y|x)$ 的函数。更进一步地，$I(X;Y)$ 与 $P(x)$ 为 $\bigcap$ 型凸函数，与 $P(y|x)$ 为 $\bigcup$ 型凸函数。以一个二元离散信源与 BSC 信道构成的通信系统为例：
$$
    \begin{bmatrix}
        X\\ \ \\ P(x)\\
    \end{bmatrix} = 
    \begin{bmatrix}
        0 & 1\\
        \ \\
        w & 1-w \triangleq \bar{w}\\
    \end{bmatrix}, \ \ \pmb{P} = 
    \begin{bmatrix}
        1-p \triangleq \bar{p} & p\\
        \ \\
        p & \bar{p}\\
    \end{bmatrix}
$$
$$
    \because \begin{align}
        \notag H(Y|X) &= \sum_{x \in X} P(x) \sum_{y \in Y} P(y|x) \log_2 \dfrac{1}{P(y|x)} = (w+\bar{w}) \left(\bar{p} \log_2 \dfrac{1}{\bar{p}} + p \log_2 \dfrac{1}{p} \right) = p \log_2 \dfrac{1}{p} + \bar{p} \log_2 \dfrac{1}{\bar{p}} \triangleq \mathcal{H} (p)\\
        \notag \ \\
        \notag H(Y) &= \sum_{y \in Y} P(y) \log_2 \dfrac{1}{P(y)} = ( w \bar{p} + \bar{w} p ) \log_2 \dfrac{1}{( w \bar{p} + \bar{w} p )} + ( 1- w \bar{p} - \bar{w} p ) \log_2 \dfrac{1}{( 1 - w \bar{p} - \bar{w} p )} \triangleq \mathcal{H} (w \bar{p} + \bar{w} p)
    \end{align}
$$
$$
    \therefore I(X;Y) = \mathcal{H} (w \bar{p} + \bar{w} p) - \mathcal{H} (p)
$$
在三维笛卡尔坐标系空间上绘制二元函数 $\mathcal{H} (w \bar{p} + \bar{w} p) - \mathcal{H} (p)$ 的图像如下所示，总体函数图像呈现马鞍面的形状：

![I的凸函数特性](/Study_Bilibili/信息论与编码基础/figures/3-5.png)

固定 $p$ 仅考虑 $w$ 时，可见 $I(X;Y)$ 是 $P(x)$ 的 $\bigcap$ 型凸函数，极大值在 $w=0.5$ 时取得，代入得 $I(X;Y)=\mathcal{H}(0.5) - \mathcal{H}(p) = 1-\mathcal{H}(p)$。换句话说，当信源符号等概分布时，信号接收端平均每个符号都能获得最大的信息量。这一凸函数特性是导出信道容量概念的依据。

固定 $w$ 仅考虑 $p$ 时，可见 $I(X;Y)$ 是 $P(y|x)$ 的 $\bigcup$ 型凸函数，极小值在 $p=0.5$ 时取得（0），极大值在 $p=0$ 或 $p=1$ 时取得（$\mathcal{H}(w)$）。前者的实际情况是，信道噪声极大（最差信道），接收端纯靠瞎猜来确定真实信息（误码率高达 50%）；后两者的情况等价于无损信道。这一凸函数特性是导出率失真函数概念的重要依据。

## 3.2 信道容量
### 3.2.1 信道容量的定义
研究信道的目的是为了获得尽可能高的信息传输率，即信道中平均每个符号能传送的信息量尽可能大。需要注意的是，“信息传输率”一词的概念可以进行扩展。当对信源进行某种变换或编码后，得到的新信源的熵可理解为变换/编码的信息传输率；信源的熵也可以视为信源的信息传输率；信源发送的信息经过信道传输后的信息传输率即为平均互信息 $I(X;Y)$。

根据上一节的相关结论，$I(X;Y)$ 与信源分布 $P(x)$、信道转移概率 $P(y|x)$ 具有确定的函数关系：$I(X;Y) \triangleq f \left[ P(x), P(y|x) \right]$，且 $I(X;Y)$ 与 $P(x)$ 为 $\bigcap$ 型凸函数关系，所以信源存在某种最佳概率分布使得 $I(X;Y)$ 达到最大，即为信道容量 $C$：
$$
    C \triangleq \underset{P(x)} {\max} \left\{ I(X;Y) \right\}
    \tag{3-14}
$$
在 3.1.3 节末尾展示的案例中，BSC 信道的信道容量为 $1-\mathcal{H}(p)$，在信源等概分布时取得，仅与信道转移概率 $p$ 有关。对于一个固定质量的信道，$C$ 是 $P(x)$ 满足某种分布后取得的理论最优值，不再随 $P(x)$ 的实际分布变化而变化，其数值大小直接反映信道质量（$p$）的高低；但是信道的信息传输率 $R$ 会受到信源分布的影响。换句话说，信道容量 $C$ 是在信道质量 $p$ 已定的情况下、信息传输率 $R$ 可达到的上限。

在中文语境下容易与**信息传输率** $R$ 混淆的一个概念是**信息传输速率** $R_t$（单位：$\rm bit/s$），$R_t$ 表示单位时间 $t$ 内平均每个符号所传输的信息量：
$$
    R_t = \dfrac{1}{t} I(X;Y)
    \tag{3-15}
$$
单位时间内信道传输的最大信息量则用 $C_t=C/t$ 表示。$C_t$ 同样可以表示信道容量的含义。脑-机接口系统中常用的 ITR 指标与 $R_t$ 的定义其实是如出一辙的。

另一方面，由于物理因素的限制，无损信道几乎是不可能实现的。当传输错误大小超过了可靠性容限时，必须采用信道编码的方法，在信源符号中添加冗余码元以便于接收端纠正错误、提高传输可靠性。因此，信息传输率与传输可靠性成为了一对基本矛盾。幸运的是，Shannon 信息理论中的有噪信道编码定理指出：对于任一信道，总存在最佳的信道编码方法，使得信息传输率 $R$ 在不超过信道容量 $C$ 时能够获得任意高的传输可靠性。

### 3.2.2 简单离散信道的信道容量
当离散信道的输入与输出之间存在确定关系或简单统计依赖关系时，该信道可称为简单离散信道，包括**无噪无损信道**、**有噪无损信道**以及**无噪有损信道**。

![简单离散信道示意图](/Study_Bilibili/信息论与编码基础/figures/3-6.png)

（1）无噪无损信道：
这种理想模型（输入端 $s$ 元）的信道转移概率以及信道转移矩阵分别为：
$$
    P(b_j|a_i) = \begin{cases}
        0, \ \ i \ne j\\
        1, \ \ i = j
    \end{cases}, \ \ \pmb{P} = \pmb{I}_r
$$
显然它的信道疑义度 $H(X|Y)$ 为 0，平均互信息 $I(X;Y)$ 为 $H(X)$ 或 $H(Y)$，信道容量为：
$$
    C = \underset{P(x)} {\max} \left\{ I(X;Y) \right\} = \underset{P(x)} {\max} \left\{ H(X) \right\} = -\sum_{i=1}^{s} \dfrac{1}{s} \log_2 \dfrac{1}{s} = \log_2 s
$$

（2）有噪无损信道：
对于图中所示的信道，其信道转移矩阵为：
$$
    \pmb{P} =
    \begin{bmatrix}
        0.5 & 0.5 & 0 & 0 & 0 & 0\\
        0 & 0 & 0.6 & 0.3 & 0.1 & 0\\
        0 & 0 & 0 & 0 & 0 & 1\\
    \end{bmatrix}
$$
每个输入符号经过信道传输后可能变成几种输出符号，因此噪声熵 $H(Y|X) > 0$；但是在接收端收到的符号能够分别确定唯一可能的发送符号（$\left\{ b_1,b_2 \right\}$、$\left\{ b_3,b_4,b_5 \right\}$、$\left\{ b_6 \right\}$ 相互没有交集），因此信道疑义度 $H(X|Y) = 0$（依然属于无损传输），其平均互信息 $I(X;Y)$ 与信道容量 $C$ 分别为（输入端 $s$ 元）：
$$
    I(X;Y) = H(X) < H(Y), \ \ \ C = \underset{P(x)} {\max} \left\{ I(X;Y) \right\} = \underset{P(x)} {\max} \left\{ H(X) \right\} = \log_2 s
$$
若 $s$ 行信道转移矩阵中每一列有且仅有一个非零元素，则该信道一定是无损信道、信息传输率 $R$ 等于信源熵 $H(X)$、信道容量 $C$ 等于 $\log_2 s$。

（3）无噪有损信道：
对于图中所示的信道，其信道转移矩阵为：
$$
    \pmb{P} =
    \begin{bmatrix}
        1 & 0 & 0\\
        0 & 1 & 0\\
        0 & 1 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1\\
        0 & 0 & 1\\
    \end{bmatrix}
$$
每个输入符号都能确定地转变为某一输出符号，因此噪声熵 $H(Y|X) = 0$。但是我们无法准确判断输入端发送的具体是什么符号，因此 $H(X|Y)>0$。假设接受端 $Y$ 有 $r$ 种符号，则 $Y$ 等概分布时 $H(Y)$ 最大，因此：
$$
    I(X;Y) = H(Y) < H(X), \ \ \ C = \underset{P(x)} {\max} \left\{ I(X;Y) \right\} = \underset{P(x)} {\max} \left\{ H(Y) \right\} = \log_2 r
$$
与有噪无损信道相反，若 $r$ 列信道转移矩阵中每一行有且仅有一个非零元素，则该信道一定是无噪信道、信息传输率 $R=H(Y)$、信道容量 $C=\log_2 r$

需要强调的是，“**无噪**”和“**无损**”是两个不同的概念：

“**无噪**”指的是输入端发送的符号 $a$ 经过信道传输后，只会输出一种符号 $b$ 并被接收端收到（$H(Y|X)=0$，允许多对一），至于 $b$ 的信息含义是否发生改变，是无法保证的。例如在发送端输入 1、接收端始终接受到 0：此时尽管信息发生了错误（真实信息已经受损），但是这个传输过程本身是**稳定**的、**结果唯一**的、**没有被噪声干扰**的，它不会在某一次传送过程中突然输出 3；

而“**无损**”指的是通过接收端获取的符号能够完整不出差错地还原发送端输入的全部信息（$H(X|Y)=0$，允许一对多）。例如向搜索引擎输入“人类男性”，接收端可能会输出各种各样的人类男性图片。这个过程每一次输出的信息都**不完全一致**（有噪声），但是不会获得表征显著的“人类女性”图片，即想要传递的**真实信息没有受损**。

之前介绍过的 BSC 和 BEC 信道，它们属于有噪有损信道，类似这种更一般情况的信道，其信道容量计算就变得十分复杂了。接下来介绍一种特殊的有噪有损信道——对称离散信道。

### 3.2.3 对称离散信道的信道容量
对称离散信道的信道转移矩阵具有对称性，$\pmb{P}$ 中的每一行（对应输出端）都由集合 $P_r = \left\{ p_1,p_2,\cdots,p_r \right\}$ 内的元素排列而成，每一列（对应输入端）由集合 $Q_s = \left\{ q_1,q_2,\cdots,q_s \right\}$ 内的元素排列而成。事实上，为了满足这种对称性，$P_r$ 和 $Q_s$ 的设计存在诸多限制。比如当 $r>s$ 时，则应有 $Q_s \subsetneqq P_r$，且 $Q_s \subseteq (P_r-Q_s)$。例如：
$$
    \pmb{P}_1 = 
    \begin{bmatrix}
        0.2 & 0.8 & 0.8 & 0.2\\
        0.8 & 0.2 & 0.2 & 0.8\\
    \end{bmatrix}, \ \ \pmb{P}_2 = 
    \begin{bmatrix}
        0.1 & 0.2 & 0.6 & 0.1\\
        0.2 & 0.6 & 0.1 & 0.2\\
    \end{bmatrix}
$$
其中 $\pmb{P}_1$ 满足对称离散性质；而尽管 $\pmb{P}_2$ 的列组合 $Q_s$ 是行组合 $P_r$ 的真子集，但不满足 $Q_s \subseteq (P_r-Q_s)$，所以不满足对称离散性质。接下来考虑一个抽象层面的对称离散信道，已知：
$$
    I(X;Y) = H(Y) - H(Y|X), \ \ H(Y|X) = -\sum_{x \in X} P(x) \sum_{y \in Y} P(y|x) \log_2 P(y|x)
$$
由于 $\pmb{P}$ 中每一行元素均取自相同的集合 $P_r$，因此 $\sum_{y \in Y} P(y|x) \log_2 P(y|x)$ 对于任意 $x$ 都一样，因此有：
$$
    H(Y|X) = -\sum_{y \in Y} P(y|x) \log_2 P(y|x) \triangleq \mathcal{H} (p_1,p_2,\cdots,p_r)
$$
注意函数标识 $\mathcal{H} (*)$，在上一节中 $\mathcal{H} (p)$ 表示对 $p$ 和 $1-p$ 分别求熵并求和。本节中 $\mathcal{H} (p_1,p_2,\cdots,p_r)$ 表示对概率集合 $\left\{ p_1,p_2,\cdots,p_r \right\}$ 中的元素分别求熵并求和。平均互信息 $I(X;Y)$ 与信道容量 $C$ 分别为：
$$
    I(X;Y) = H(Y)- \mathcal{H} (p_1,p_2,\cdots,p_r), \ \ \ C = \underset{P(x)} {\max} \left\{ H(Y) \right\} - \mathcal{H} (p_1,p_2,\cdots,p_r)
    \tag{3-16}
$$
接收端输出的符号共 $r$ 个，所以 $H(Y) \leqslant \log_2 r$，当且仅当输出符号等概分布时取等号。对于一般的离散信道（信道转移矩阵），使得 $P(y)$ 满足等概分布的 $P(x)$ 是不一定存在的。但对于对称离散信道而言，只要输入 $X$ 满足等概分布，则输出 $Y$ 必然也满足等概分布。考虑输出分布 $P(y)$：
$$
    P(y) = \sum_{x \in X} P(xy) = \sum_{x \in X} P(x) P(y|x)
$$
若输入分布 $P(x)$ 等概率，即 $P(x)=\dfrac{1}{s}$，则有：
$$
    P(y) = \dfrac{1}{s} \sum_{x \in X} P(y|x)
$$
由于对称离散信道的每一列之和 $\sum_{i=1}^s q_i$ 都是常数，所以不论输入 $x$ 是什么，$\sum_{x \in X} P(y|x)$ 同样也是常数 $\sum_{i=1}^s q_i$，而与输出值 $y$ 无关。因此 $Y$ 也是等概分布的（$P(y) = \dfrac{1}{r}$）。所以对称离散信道的信道容量 $C$ 如下所示。该参数仅与信道转移矩阵的行向量（输出端）集合 $P_r = \left\{ p_1,p_2,\cdots,p_r \right\}$ 有关：
$$
    C = \log_2 r - \mathcal{H} (p_1,p_2,\cdots,p_r)
    \tag{3-17}
$$
提到对称，不妨设想这样一种信道：输入端与输出端的符号个数相等（$s=r$），每个输入符号 $x_i$ 输出正确对应符号 $y_i$ 的概率为 $1-p \triangleq \bar{p}$，其余 $r-1$ 个符号的错发概率均为 $\dfrac{p}{r-1}$。这种信道称为**强对称信道**或**均匀信道**，信道转移矩阵 $\pmb{P}$ 为：
$$
    \pmb{P} = \begin{bmatrix}
        \bar{p} & \dfrac{p}{r-1} & \cdots & \dfrac{p}{r-1}\\
        \ \\
        \dfrac{p}{r-1} & \bar{p} & \cdots & \dfrac{p}{r-1}\\
        \ \\
        \vdots & \vdots & \ddots & \vdots\\
        \ \\
        \dfrac{p}{r-1} & \dfrac{p}{r-1} & \cdots & \bar{p}\\
    \end{bmatrix}
    \tag{3-18}
$$
信道容量 $C$ 为：
$$
    C = \log_2 r - \mathcal{H} \left( 1-p, \dfrac{p}{r-1},\cdots,\dfrac{p}{r-1} \right) = \log_2 r + \bar{p} \log_2 \bar{p} + \sum_{i=1}^{r-1} \dfrac{p}{r-1} \log_2 \dfrac{p}{r-1} = \log_2 r - p \log_2 (r-1) + \mathcal{H} (p)
    \tag{3-19}
$$

### 3.2.4 扩展信道与并联信道的信道容量
**（1）离散无记忆 $N$ 次扩展信道**
假设离散无记忆信道 $\mathcal{P}$ 的 $s$ 个输入符号取自集合 $A=\left\{ a_1,a_2,\cdots,a_s \right\}$，$r$ 个输出符号取自集合 $B=\left\{ b_1,b_2,\cdots,b_r \right\}$，信道转移矩阵 $\pmb{P}$ 为：
$$
    \pmb{P} = 
    \begin{bmatrix}
        P_{1,1} & P_{1,2} & \cdots & P_{1,r}\\
        \ \\
        P_{2,1} & P_{2,2} & \cdots & P_{2,r}\\
        \ \\
        \vdots & \vdots & \ddots & \vdots\\
        \ \\
        P_{s,1} & P_{s,2} & \cdots & P_{s,r}\\
    \end{bmatrix} \in \mathbb{R}^{s \times r}, \ \ \ \sum_{j=1}^{r} P_{i,j} = 1, \ i=1,2,\cdots,s
    \tag{3-20}
$$
首先简单解释一下什么叫扩展信道：假如有一个信道，其信源 $X$ 的输入是单一长度的 “0-1” 二进制码，输出 $Y$ 为 “x-y”字符，该信道的二次扩展，就是信源输入 $X_i X_j$ 变成了两位长度的二进制码 “00”、“01”、“10” 和 “11”，输出 $Y_i Y_j$ 为字符串 “xx”、“xy”、“yx” 和 “yy”；$N$ 次扩展时，信源 $\pmb{X} = (X_1 X_2 \cdots X_N)$、接收端 $\pmb{Y} = (Y_1 Y_2 \cdots Y_N)$ 的长度都相应地扩增了 $N-1$ 倍，但其基本组成元素与原始信源与输出一致。

回到信道 $\mathcal{P}$，其 $N$ 次扩展信道 $\mathcal{P}^N$ 输入向量 $\pmb{\alpha}_i = \left( a_{i_1},a_{i_2},\cdots,a_{i_N} \right)$ 的可能取值变为 $s^N$ 个，输出向量 $\pmb{\beta}_j = \left( b_{j_1},b_{j_2},\cdots,b_{j_N} \right)$ 的可能取值变为 $r^N$ 个，信道转移矩阵为：
$$
    \pmb{P}^N = 
    \begin{bmatrix}
        P_{1,1}^N & P_{1,2}^N & \cdots & P_{1,r^N}\\
        \ \\
        P_{2,1}^N & P_{2,2}^N & \cdots & P_{2,r^N}\\
        \ \\
        \vdots & \vdots & \ddots & \vdots\\
        \ \\
        P_{s^N,1} & P_{s^N,2} & \cdots & P_{s^N,r^N}\\
    \end{bmatrix} \in \mathbb{R}^{s^N \times r^N}, \ \ \ \sum_{j=1}^{r^N} P_{i,j}^N = 1, \ i=1,2,\cdots,s^N
    \tag{3-21}
$$
其中：
$$
    P_{i,j} = P \left( \pmb{\beta}_j | \pmb{\alpha}_i \right) = P \left( b_{j_1} b_{j_2} \cdots b_{j_N} | a_{i_1} a_{i_2} \cdots a_{i_N} \right) = \prod_{k=1}^{N} P \left( b_{j_k} | a_{i_k} \right), \ \ \ i_k \in \left\{ 1,2,\cdots,s^N \right\}, \ j_k \in \left\{ 1,2,\cdots,r^N \right\}
    \tag{3-22}
$$
扩展信道 $\mathcal{P}^N$ 的平均互信息满足：
$$
    I \left( X^N;Y^N \right) \leqslant \sum_{i=1}^N I \left( X_i;Y_i \right)
    \tag{3-23}
$$
式（3-23）与熵的独立界性质有关：$H(X_1 X_2 \cdots X_N) \leqslant \sum_{i=1}^N H(X_i)$，当随机事件 $X_1$、$X_2$ 等独立同分布时，等号成立。此处当信源 $\pmb{X} = (X_1 X_2 \cdots X_N)$ 为离散无记忆时，式（3-23）的等号成立。进一步地当各分信源 $X_i$ 取得最佳分布时（所有信源性质一样），信道容量为：
$$
    C^N = \sum_{i=1}^N \underset{P(x_i)} {\max} I(X_i;Y_i) = \sum_{i=1}^N C_i
    \tag{3-24}
$$
若已求得离散无记忆信道 $\mathcal{P}$ 的信道容量 $C$，则有 $C_i = C$，其 $N$ 次扩展 $\mathcal{P}^N$ 的信道容量 $C_N$ 为 $NC$。再强调一次，达到这个参数需要满足两个前提条件：首先输入信源需满足离散无记忆，其次每一输入变量 $X_i$ 都要达到最佳分布。

**（2）并联信道**
并联组合是实际应用中常见的信道组合方式，具体分为三种：

![并联信道方法](/Study_Bilibili/信息论与编码基础/figures/3-7.png)

接下来简单介绍三种并联方式：

（a）输入并接信道：
该类信道由 $N$ 个子信道组成，具有相同的输入符号集合 $X = \left\{ x_1,x_2,\cdots,x_s \right\}$，每个信道的输出共同组成输出向量（符号组或字符串）$\pmb{Y} = (Y_1 Y_2 \cdots Y_N)$。这种信道具有“单输入、多输出”性质，在物理意义上类似于对同一随机事件利用不同工具进行多次观测，虽然信息传输效率比较低，但在成本可以接受的情况下，这种设计能够稳定提高信道传输的可靠性。

通过输入并接信道传输的信息为：
$$
    \begin{align}
        \notag I(X;Y_1 Y_2 \cdots Y_N) &= I(X;Y_1) + I(X;Y_2|Y_1) + I(X;Y_3|Y_1 Y_2) + \cdots + I(X;Y_N|Y_1 Y_2 \cdots Y_{N-1})\\
        \notag \ \\
        \notag &= I(X;Y_2) + I(X;Y_1|Y_2) + I(X;Y_3|Y_1 Y_2) + \cdots + I(X;Y_N|Y_1 Y_2 \cdots Y_{N-1})\\
        \notag \ \\
        \notag &= \cdots\\
        \notag \ \\
        \notag &= I(X;Y_N) + I(X;Y_1|Y_N) + I(X;Y_2|Y_1 Y_N) + \cdots + I(X;Y_{N-1}|Y_1 Y_2 \cdots Y_{N-2} Y_N)\\
    \end{align}
    \tag{3-25}
$$
根据等式右边的首项变量可知，输入并接信道的信道容量必然大于任意一个子信道的信道容量。不过由于信道转移概率矩阵过于复杂，想要精确求解信道容量的数值比较困难。但是信道容量的上界很好确定：
$$
    I(X;Y_1 Y_2 \cdots Y_N) = H(X) - H(X|Y_1 Y_2 \cdots Y_N) \leqslant H(X), \ \ \ C \leqslant \underset{P(x)} {\max} H(X)
    \tag{3-26}
$$

（b）并用信道：
该类信道的子信道输入、输出符号彼此独立，分别对应于输入和输出向量中的一个分量。这一模型与之前介绍的离散无记忆 $N$ 次扩展信道很相似：
$$
    P(y_1 y_2 \cdots y_N | x_1 x_2 \cdots x_N) = \prod_{i=1}^N P(y_i | x_i)
    \tag{3-27}
$$
类比可知，并用信道的信道容量为 $C = \sum_{i=1}^N C_i$，即等于各组成信道的信道容量之和。

（c）和信道：
和信道的 $N$ 个子信道是完全独立的，既没有在输入端并接，也没有同时使用，仅仅是被视为一个整体、可任意选用子信道。在实际应用中，这种信道通常用于备份场景，例如备用通道、系统备份等。假设第 $n$ 个子信道的发送端输入符号 $a_{s_n}$ 个数为 $S_n$，接收端输出符号 $b_{r_n}$ 个数为 $R_n$，转移概率为 $P_n \left( b_{r_n} | a_{s_n} \right)$，（$s_n = 1,2,\cdots,S_n$，$r_n = 1,2,\cdots,R_n$）；各子信道的转移矩阵分别为 $\pmb{P}_1$、$\pmb{P}_2$、$\cdots$、$\pmb{P}_N$，和信道的转移矩阵 $\pmb{P}$ 是各子信道矩阵组成的分块对角阵：
$$
    \pmb{P} = \begin{bmatrix}
        \pmb{P}_1\\
        \ & \pmb{P}_2\\
        \ & \ & \ddots \\
        \ & \ & \ & \pmb{P}_N
    \end{bmatrix}
    \tag{3-28}
$$
（为什么不接着写信道容量了呢，因为这本参考教材实在是太烂了，公式里左丢一个下标右出一个新符号，根本看不下去，没想到国科大出品也有垃圾。等我看完以后换一本再看看）